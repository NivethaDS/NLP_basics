{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\1mscdsa20/nltk_data', 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data', 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\1mscdsa20\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1mscdsa20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path.append('E:\\\\nltk_data')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->>Normally we can split the sentence into tokens by using the sent_tokenize\n",
    "->>\n",
    "Tokens - Python breaks each logical line into the sequence of elementery lexical components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.How many word tokens there are in gift_of_magi file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # The nltk package can be imported \n",
    "from nltk.tokenize import word_tokenize # from the package we import the word_tokenize \n",
    "with open('gift-of-magi.txt', 'r') as file:#here we are try to oprn the file by using the with open keyword \n",
    "    text = file.read()#read th file that we have opened\n",
    "words = word_tokenize(text)#at last we tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens in th given file: 2468\n"
     ]
    }
   ],
   "source": [
    "tokens=len(words)#Count odf a word in a given file can be stored in a Variable Tokens\n",
    "print(\"Number of Tokens in th given file:\",tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.How many word types there are in a given Ffile(word types are a unique set of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word types: 828\n"
     ]
    }
   ],
   "source": [
    "types=len(set(words))\n",
    "print(f\"Number of word types: {types}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Top 20 most frequent words and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 141), ('the', 109), (',', 104), ('and', 75), ('a', 65), ('of', 51), ('to', 41), ('``', 30), (\"''\", 30), ('it', 29), ('was', 27), ('Jim', 26), ('she', 25), ('in', 24), ('her', 24), ('had', 21), ('that', 20), ('Della', 20), ('for', 20), ('at', 19)]\n",
      ".: 141\n",
      "the: 109\n",
      ",: 104\n",
      "and: 75\n",
      "a: 65\n",
      "of: 51\n",
      "to: 41\n",
      "``: 30\n",
      "'': 30\n",
      "it: 29\n",
      "was: 27\n",
      "Jim: 26\n",
      "she: 25\n",
      "in: 24\n",
      "her: 24\n",
      "had: 21\n",
      "that: 20\n",
      "Della: 20\n",
      "for: 20\n",
      "at: 19\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist#frequency distribution object were imported\n",
    "freq_dist=FreqDist(words)\n",
    "top_20_words=freq_dist.most_common(20)#top 20 words can be stored here\n",
    "print(top_20_words)\n",
    "for word,count in top_20_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Words that are at least 10 characters long and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 56 samples and 70 outcomes>\n",
      "LONG WORDS AND THEIR COUNT\n",
      "eighty-seven: 3\n",
      "bulldozing: 1\n",
      "imputation: 1\n",
      "instigates: 1\n",
      "reflection: 4\n",
      "predominating: 1\n",
      "description: 2\n",
      "mendicancy: 1\n",
      "letter-box: 1\n",
      "appertaining: 1\n",
      "Dillingham: 6\n",
      "prosperity: 1\n",
      "contracting: 1\n",
      "unassuming: 1\n",
      "introduced: 1\n",
      "calculated: 1\n",
      "pier-glass: 2\n",
      "longitudinal: 1\n",
      "conception: 1\n",
      "brilliantly: 1\n",
      "possessions: 1\n",
      "grandfather: 1\n",
      "depreciate: 1\n",
      "ransacking: 1\n",
      "proclaiming: 1\n",
      "meretricious: 1\n",
      "ornamentation: 1\n",
      "Twenty-one: 1\n",
      "intoxication: 1\n",
      "generosity: 1\n",
      "tremendous: 1\n",
      "close-lying: 1\n",
      "wonderfully: 2\n",
      "critically: 1\n",
      "frying-pan: 1\n",
      "twenty-two: 1\n",
      "expression: 2\n",
      "disapproval: 1\n",
      "sentiments: 1\n",
      "laboriously: 1\n",
      "inconsequential: 1\n",
      "difference: 1\n",
      "mathematician: 1\n",
      "illuminated: 1\n",
      "hysterical: 1\n",
      "necessitating: 1\n",
      "employment: 1\n",
      "comforting: 1\n",
      "worshipped: 1\n",
      "tortoise-shell: 1\n",
      "possession: 1\n",
      "adornments: 1\n",
      "duplication: 1\n",
      "uneventful: 1\n",
      "sacrificed: 1\n",
      "Everywhere: 1\n"
     ]
    }
   ],
   "source": [
    "long_word=[word for word in words if len(word) >= 10]#character of a word greater than 10 can be extract and stored in a variable  \n",
    "long_word_dist=FreqDist(long_word)\n",
    "print(long_word_dist)\n",
    "print(\"LONG WORDS AND THEIR COUNT\")\n",
    "for word,count in long_word_dist.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.10+ characters-long words that occur at least twice, sorted from most frequent to least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dillingham: 6\n",
      "reflection: 4\n",
      "eighty-seven: 3\n",
      "description: 2\n",
      "pier-glass: 2\n",
      "wonderfully: 2\n",
      "expression: 2\n"
     ]
    }
   ],
   "source": [
    "long_words_twice=[word for word, count in long_words_dist.items() if count >= 2]\n",
    "long_words_twice.sort(key=lambda x:long_words_dist[x], reverse=True)#lambda function can be used to reduce the line of code\n",
    "for word in long_words_twice:\n",
    "    print(f\"{word}:{long_words_dist[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
